{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise relevant packages\n",
    "\n",
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Preprocessing\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelling\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, DistilBertTokenizerFast\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USER INPUT: Pick Models and Test Sets for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR: this should be the only place that we specify which models we want to test and which test sets we want to test them on\n",
    "model_list = ['BERT_combined_unweighted', 'BERT_combined_weighted']\n",
    "last_model = \"120221\"\n",
    "what_pct = 0.01 # what pct of the datasets to use for eval\n",
    "test_data_list = ['davidson2017', 'dynabench2021','founta2018','combined'] # example\n",
    "\n",
    "# PR: from here on out, there should be no user input neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_combined_unweighted\n",
      "BERT_combined_weighted\n"
     ]
    }
   ],
   "source": [
    "# PR: DG, you should load the tokenizer as part of the model here already so you can use it further down\n",
    "\n",
    "def loadModels(model_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to load models and return them in the correct format\n",
    "    \n",
    "    model_list : list \n",
    "            list of model names\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    for model_name in model_list:\n",
    "        models[model_name] = BertForSequenceClassification.from_pretrained(f'./Models/{model_name}_{last_model}/Final')\n",
    "        \n",
    "       \n",
    "    trainer = {}\n",
    "\n",
    "    for model_name in models:\n",
    "        print(model_name)\n",
    "        trainer[model_name] = Trainer(\n",
    "            model=models[model_name],         \n",
    "            args=TrainingArguments(\n",
    "                output_dir= f'./Models/{model_name}/Test',\n",
    "                per_device_eval_batch_size = 64)\n",
    "    )\n",
    "        \n",
    "    return trainer\n",
    "\n",
    "trainer = loadModels(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1.) Evaluate on Held-Out Test Sets\n",
    "## Load Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR: DG, please adapt function to load correct data based on model list (might need simple regex) and then return it in the correct format\n",
    "# using the full model names and regexing the dataset name will give us a lot more flexibility down the line\n",
    "\n",
    "def LoadTestSet(test_data_list = test_data_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to load held-out test sets and return them in the correct format\n",
    "    \n",
    "    model_list : list \n",
    "            list of model names\n",
    "    \"\"\"\n",
    "    \n",
    "    test_texts, test_labels, test_encodings, test_dataset = {}, {}, {},{}\n",
    "\n",
    "    # load Tokenizer\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    for dataset in test_data_list:\n",
    "        data_test = pd.read_csv(f\"/home/ec2-user/Projects/v0.1/Data/{dataset}/{dataset}_test.csv\",lineterminator=\"\\n\",)#training_data[dataset].copy()#.sample(n=100, random_state=123)#df_train[dataset].text.astype(\"string\").tolist()\n",
    "        data_test = data_test.copy().sample(frac=what_pct, random_state=123)\n",
    "        test_texts[dataset] = data_test.text.astype(\"string\").tolist()\n",
    "        test_labels[dataset] = data_test.label.tolist()\n",
    "        test_encodings[dataset] = tokenizer(test_texts[dataset], truncation=True, padding=True)\n",
    "        test_dataset[dataset] = HateDataset(test_encodings[dataset], test_labels[dataset])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    #print(df_raw[dataset])\n",
    "    #for dset in dataset:\n",
    "    #    test_labels[dset] = df_test[dset].label.tolist()\n",
    "    #    test_texts[dset] = df_test[dset].text.astype(\"string\").tolist()\n",
    "    #    #print(test_labels[dset])\n",
    "    #    #print(test_texts[dset])\n",
    "    #    test_encodings[dset] = tokenizer(test_texts[dset], truncation=True, padding=True)\n",
    "    #    #print(test_encodings[dset])\n",
    "    #    test_dataset[dset] = HateDataset(test_encodings[dset], test_labels[dset])\n",
    "\n",
    "    return test_dataset,test_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Predictions on Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelPrediction(trainer, test_dataset,index = None):\n",
    "    \"\"\"\n",
    "    Helper function to get the model's prediction as a dictionnary of results indexed by dataset \n",
    "    \n",
    "    \n",
    "    trainer : dict\n",
    "            dict of all the models, each corresponding to one dataset\n",
    "    test_dataset : dict\n",
    "            dict of all the test subsets relative to each data set\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    result_dfs = {}\n",
    "    print(trainer.keys())\n",
    "\n",
    "    for dataset in trainer:\n",
    "        print('Evaluating weighted {} BERT model on test data'.format(dataset))\n",
    "        results[dataset] = trainer[dataset].predict(test_dataset[dataset])\n",
    "        \n",
    "        new_df = pd.DataFrame(np.argmax(results[dataset].predictions,1),columns=[f\"pred_BERT_{dataset}_weighted\"], index = index )\n",
    "        result_dfs[dataset]=new_df\n",
    "        \n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Model Performance on Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateOnTest(trainer, test_dataset,test_labels):\n",
    "    \"\"\"\n",
    "    Helper function to evaluate on a held out test data \n",
    "    \n",
    "    trainer : dict\n",
    "            dict of all the models, each corresponding to one dataset\n",
    "    test_dataset : dict\n",
    "            dict of all the test subsets relative to each data set\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    print(test_dataset.keys())\n",
    "    print(test_dataset)\n",
    "    \n",
    "    print(trainer.keys())\n",
    "    print(trainer)\n",
    "\n",
    "    for model in trainer:\n",
    "        for dataset in test_dataset.keys():\n",
    "            print('Evaluating weighted {} BERT model on test data'.format(dataset))\n",
    "            results[f\"{model}_{dataset}\"] = trainer[model].predict(test_dataset[dataset])\n",
    "            for metric in results[f\"{model}_{dataset}\"].metrics:\n",
    "                print(metric, results[f\"{model}_{dataset}\"].metrics['{}'.format(metric)])\n",
    "    pred_labels={}\n",
    "\n",
    "    for model in trainer:\n",
    "        for dataset in test_dataset.keys():\n",
    "\n",
    "            preds=[]\n",
    "\n",
    "            for row in results[f\"{model}_{dataset}\"][0]:\n",
    "                preds.append(int(np.argmax(row)))\n",
    "\n",
    "            pred_labels[f\"{model}_{dataset}\"] = pd.Series(preds)\n",
    "\n",
    "    # print classification reports for each model\n",
    "\n",
    "    for model in trainer:\n",
    "        for dataset in test_dataset.keys():\n",
    "            print(f\"{model}_{dataset}\".upper())\n",
    "            print(test_labels[dataset],pred_labels[f\"{model}_{dataset}\"])\n",
    "            print(classification_report(test_labels[dataset],pred_labels[f\"{model}_{dataset}\"]))\n",
    "            print()\n",
    "    # f1 scores\n",
    "    for model in trainer:\n",
    "        for dataset in test_dataset.keys():\n",
    "            print(f\"{model}_{dataset}\".upper())\n",
    "            for average in ['micro', 'macro', 'weighted']:\n",
    "                print('{} F1 score: {:.2%}'.format(average, f1_score(test_labels[dataset],pred_labels[f\"{model}_{dataset}\"], average=average)))\n",
    "            print()\n",
    "    # distribution of predictions\n",
    "    for model in trainer:\n",
    "        for dataset in test_dataset.keys():\n",
    "            print(f\"{model}_{dataset}\".upper())\n",
    "            print(pred_labels[f\"{model}_{dataset}\"].value_counts())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR: I think the \"test set\" section could end here and the below code up to \"2.) Evaluate on HateCheck\" could be moved into the functions\n",
    "\n",
    "The three lines below actually run the evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_combined_unweighted\n",
      "BERT_combined_weighted\n",
      "dict_keys(['davidson2017', 'dynabench2021', 'founta2018', 'combined'])\n",
      "{'davidson2017': <__main__.HateDataset object at 0x7f9c7fe25c70>, 'dynabench2021': <__main__.HateDataset object at 0x7f9c7fe25f10>, 'founta2018': <__main__.HateDataset object at 0x7f9c7fe25df0>, 'combined': <__main__.HateDataset object at 0x7f9d20014160>}\n",
      "dict_keys(['BERT_combined_unweighted', 'BERT_combined_weighted'])\n",
      "{'BERT_combined_unweighted': <transformers.trainer.Trainer object at 0x7f9d200525e0>, 'BERT_combined_weighted': <transformers.trainer.Trainer object at 0x7f9ccd47bfd0>}\n",
      "Evaluating weighted davidson2017 BERT model on test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss 0.021819284185767174\n",
      "eval_runtime 1.6145\n",
      "eval_samples_per_second 15.485\n",
      "Evaluating weighted dynabench2021 BERT model on test data\n",
      "eval_loss 0.2695775628089905\n",
      "eval_runtime 3.5612\n",
      "eval_samples_per_second 8.424\n",
      "Evaluating weighted founta2018 BERT model on test data\n",
      "eval_loss 0.08650258928537369\n",
      "eval_runtime 3.6949\n",
      "eval_samples_per_second 27.065\n",
      "Evaluating weighted combined BERT model on test data\n",
      "eval_loss 0.15805402398109436\n",
      "eval_runtime 10.4585\n",
      "eval_samples_per_second 14.821\n",
      "Evaluating weighted davidson2017 BERT model on test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss 0.03562268614768982\n",
      "eval_runtime 1.5261\n",
      "eval_samples_per_second 16.382\n",
      "Evaluating weighted dynabench2021 BERT model on test data\n",
      "eval_loss 0.43717288970947266\n",
      "eval_runtime 3.5073\n",
      "eval_samples_per_second 8.554\n",
      "Evaluating weighted founta2018 BERT model on test data\n",
      "eval_loss 0.08916343748569489\n",
      "eval_runtime 3.7046\n",
      "eval_samples_per_second 26.993\n",
      "Evaluating weighted combined BERT model on test data\n",
      "eval_loss 0.34621480107307434\n",
      "eval_runtime 10.3815\n",
      "eval_samples_per_second 14.93\n",
      "BERT_COMBINED_UNWEIGHTED_DAVIDSON2017\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    1\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        24\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_DYNABENCH2021\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1] 0     1\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     1\n",
      "9     0\n",
      "10    1\n",
      "11    1\n",
      "12    0\n",
      "13    1\n",
      "14    1\n",
      "15    0\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "20    1\n",
      "21    1\n",
      "22    1\n",
      "23    0\n",
      "24    1\n",
      "25    1\n",
      "26    0\n",
      "27    0\n",
      "28    1\n",
      "29    1\n",
      "dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71         8\n",
      "           1       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.79      0.81      0.79        30\n",
      "weighted avg       0.84      0.83      0.84        30\n",
      "\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_FOUNTA2018\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Length: 100, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        96\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.99      0.75      0.83       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_COMBINED\n",
      "[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "150    0\n",
      "151    0\n",
      "152    1\n",
      "153    0\n",
      "154    0\n",
      "Length: 155, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       134\n",
      "           1       0.77      0.81      0.79        21\n",
      "\n",
      "    accuracy                           0.94       155\n",
      "   macro avg       0.87      0.89      0.88       155\n",
      "weighted avg       0.94      0.94      0.94       155\n",
      "\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_DAVIDSON2017\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    1\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        24\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_DYNABENCH2021\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1] 0     1\n",
      "1     1\n",
      "2     1\n",
      "3     0\n",
      "4     1\n",
      "5     0\n",
      "6     1\n",
      "7     1\n",
      "8     0\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    0\n",
      "19    1\n",
      "20    1\n",
      "21    1\n",
      "22    1\n",
      "23    0\n",
      "24    0\n",
      "25    1\n",
      "26    0\n",
      "27    0\n",
      "28    1\n",
      "29    1\n",
      "dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62         8\n",
      "           1       0.86      0.86      0.86        22\n",
      "\n",
      "    accuracy                           0.80        30\n",
      "   macro avg       0.74      0.74      0.74        30\n",
      "weighted avg       0.80      0.80      0.80        30\n",
      "\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_FOUNTA2018\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Length: 100, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98        96\n",
      "           1       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.79      0.86      0.83       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_COMBINED\n",
      "[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "150    0\n",
      "151    0\n",
      "152    1\n",
      "153    0\n",
      "154    0\n",
      "Length: 155, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       134\n",
      "           1       0.56      0.71      0.63        21\n",
      "\n",
      "    accuracy                           0.88       155\n",
      "   macro avg       0.75      0.81      0.78       155\n",
      "weighted avg       0.90      0.88      0.89       155\n",
      "\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_DAVIDSON2017\n",
      "micro F1 score: 100.00%\n",
      "macro F1 score: 100.00%\n",
      "weighted F1 score: 100.00%\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_DYNABENCH2021\n",
      "micro F1 score: 83.33%\n",
      "macro F1 score: 79.48%\n",
      "weighted F1 score: 83.63%\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_FOUNTA2018\n",
      "micro F1 score: 98.00%\n",
      "macro F1 score: 82.82%\n",
      "weighted F1 score: 97.68%\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_COMBINED\n",
      "micro F1 score: 94.19%\n",
      "macro F1 score: 87.85%\n",
      "weighted F1 score: 94.25%\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_DAVIDSON2017\n",
      "micro F1 score: 100.00%\n",
      "macro F1 score: 100.00%\n",
      "weighted F1 score: 100.00%\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_DYNABENCH2021\n",
      "micro F1 score: 80.00%\n",
      "macro F1 score: 74.43%\n",
      "weighted F1 score: 80.00%\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_FOUNTA2018\n",
      "micro F1 score: 97.00%\n",
      "macro F1 score: 82.55%\n",
      "weighted F1 score: 97.16%\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_COMBINED\n",
      "micro F1 score: 88.39%\n",
      "macro F1 score: 77.81%\n",
      "weighted F1 score: 88.98%\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_DAVIDSON2017\n",
      "0    24\n",
      "1     1\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_DYNABENCH2021\n",
      "1    21\n",
      "0     9\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_FOUNTA2018\n",
      "0    98\n",
      "1     2\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_UNWEIGHTED_COMBINED\n",
      "0    133\n",
      "1     22\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_DAVIDSON2017\n",
      "0    24\n",
      "1     1\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_DYNABENCH2021\n",
      "1    22\n",
      "0     8\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_FOUNTA2018\n",
      "0    95\n",
      "1     5\n",
      "dtype: int64\n",
      "\n",
      "BERT_COMBINED_WEIGHTED_COMBINED\n",
      "0    128\n",
      "1     27\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset,test_labels = LoadTestSet()\n",
    "trainer = loadModels(model_list=model_list)\n",
    "evaluateOnTest(trainer, test_dataset,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rewire",
   "language": "python",
   "name": "rewire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
