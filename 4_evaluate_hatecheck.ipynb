{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1.) Evaluate on HateCheck\n",
    "## Load HateCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PR: DG, please adapt this function to load HateCheck and return it in the correct format for further evaluation.\n",
    "#PR: There is a lot of unneccessary code in here. We are loading HateCheck as a whole and it would make things more clear if the var names reflected that\n",
    "\n",
    "def LoadHateCheck():\n",
    "\n",
    "    \"\"\"\n",
    "    Function to load HateCheck data\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train, df_valtest, df_val, df_test = {}, {}, {}, {}\n",
    "    test_texts, test_labels={},{}\n",
    "    test_encodings = {}\n",
    "    test_dataset = {}\n",
    "    \n",
    "    if len(dataset) == 1:\n",
    "\n",
    "\n",
    "        #df_test[dataset]=df_raw\n",
    "\n",
    "\n",
    "        #print(df_raw[dataset])\n",
    "        for dset in dataset:\n",
    "            test_labels[dset] = df_raw.label.tolist()\n",
    "            test_texts[dset] = df_raw.text.astype(\"string\").tolist()\n",
    "            #print(test_labels[dset])\n",
    "            #print(test_texts[dset])\n",
    "            test_encodings[dset] = tokenizer(test_texts[dset], truncation=True, padding=True)\n",
    "            #print(test_encodings[dset])\n",
    "            test_dataset[dset] = HateDataset(test_encodings[dset], test_labels[dset])\n",
    "\n",
    "        return test_dataset\n",
    "    else : \n",
    "        #print(df_raw[dataset])\n",
    "        for dset in dataset:\n",
    "            test_labels[dset] = df_raw[dset].label.tolist()\n",
    "            test_texts[dset] = df_raw[dset].text.astype(\"string\").tolist()\n",
    "            #print(test_labels[dset])\n",
    "            #print(test_texts[dset])\n",
    "            test_encodings[dset] = tokenizer(test_texts[dset], truncation=True, padding=True)\n",
    "            #print(test_encodings[dset])\n",
    "            test_dataset[dset] = HateDataset(test_encodings[dset], test_labels[dset])\n",
    "\n",
    "        return test_dataset\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined\n",
      "Evaluating weighted combined BERT model on test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PR: Hatecheck should be loaded in the previous function, not this one.\n",
    "def evaluate_on_hatecheck(hatecheck_path =\"./Data/Test Suite/hatecheck_final_ACL.csv\"):\n",
    "    hatecheck_df = pd.read_csv(hatecheck_path, index_col=0)\n",
    "    dataframe = hatecheck_df.copy()\n",
    "    dataframe.rename({\"test_case\":\"text\",\"label_gold\":\"label\"},axis = 1,inplace = True)\n",
    "    dataframe.label.replace({'hateful': 1, 'non-hateful': 0}, inplace = True)\n",
    "    case_id = dataframe.case_id.tolist()\n",
    "    trainer = getModels([\"combined\"])\n",
    "     \n",
    "    test_dataset = setupHatecheckTestDataset(df_raw = dataframe)\n",
    "    \n",
    "    \n",
    "    #print(test_dataset[\"combined\"])\n",
    "    \n",
    "    #print(trainer[\"combined\"].predict(test_dataset[\"combined\"]))\n",
    "    \n",
    "    result_df = getModelPrediction(trainer, test_dataset,index=case_id)\n",
    "    \n",
    "    result_df[\"combined\"] = result_df[\"combined\"].reset_index().rename({\"index\":\"case_id\"},axis = 1)\n",
    "    result_df[\"combined\"].pred_BERT_combined_weighted.replace({1:\"hateful\",0:\"non-hateful\"},inplace = True)\n",
    "    hatecheck_df = hatecheck_df.merge(result_df[\"combined\"], how = 'right', on = 'case_id')\n",
    "    return hatecheck_df\n",
    "    #print()\n",
    "hatecheck_df = evaluate_on_hatecheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Accuracy by Functionality across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of models to evaluate\n",
    "models_eval = [\"BERT_combined_weighted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionality</th>\n",
       "      <th>case_id</th>\n",
       "      <th>test_case</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "      <th>pred_BERT_combined_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate women.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate trans people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>3</td>\n",
       "      <td>I hate gay people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>4</td>\n",
       "      <td>I hate black people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>5</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723</th>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3897</td>\n",
       "      <td>Gay people are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3898</td>\n",
       "      <td>Black people are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3899</td>\n",
       "      <td>Disabled people are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3900</td>\n",
       "      <td>Muslims are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3901</td>\n",
       "      <td>Immigrants are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "      <td>non-hateful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3728 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          functionality  case_id                                    test_case  \\\n",
       "0     derog_neg_emote_h        1                               I hate women.    \n",
       "1     derog_neg_emote_h        2                        I hate trans people.    \n",
       "2     derog_neg_emote_h        3                          I hate gay people.    \n",
       "3     derog_neg_emote_h        4                        I hate black people.    \n",
       "4     derog_neg_emote_h        5                     I hate disabled people.    \n",
       "...                 ...      ...                                          ...   \n",
       "3723       spell_leet_h     3897       Gay people are all complete 4ssholes.    \n",
       "3724       spell_leet_h     3898     Black people are all complete 4ssholes.    \n",
       "3725       spell_leet_h     3899  Disabled people are all complete 4ssholes.    \n",
       "3726       spell_leet_h     3900          Muslims are all complete 4ssholes.    \n",
       "3727       spell_leet_h     3901       Immigrants are all complete 4ssholes.    \n",
       "\n",
       "     label_gold     target_ident direction focus_words focus_lemma  \\\n",
       "0       hateful            women   general        hate        hate   \n",
       "1       hateful     trans people   general        hate        hate   \n",
       "2       hateful       gay people   general        hate        hate   \n",
       "3       hateful     black people   general        hate        hate   \n",
       "4       hateful  disabled people   general        hate        hate   \n",
       "...         ...              ...       ...         ...         ...   \n",
       "3723    hateful       gay people   general    4ssholes     asshole   \n",
       "3724    hateful     black people   general    4ssholes     asshole   \n",
       "3725    hateful  disabled people   general    4ssholes     asshole   \n",
       "3726    hateful          Muslims   general    4ssholes     asshole   \n",
       "3727    hateful       immigrants   general    4ssholes     asshole   \n",
       "\n",
       "      ref_case_id  ref_templ_id  templ_id  \\\n",
       "0             NaN           NaN         1   \n",
       "1             NaN           NaN         1   \n",
       "2             NaN           NaN         1   \n",
       "3             NaN           NaN         1   \n",
       "4             NaN           NaN         1   \n",
       "...           ...           ...       ...   \n",
       "3723       1232.0         256.0       866   \n",
       "3724       1233.0         256.0       866   \n",
       "3725       1234.0         256.0       866   \n",
       "3726       1235.0         256.0       866   \n",
       "3727       1236.0         256.0       866   \n",
       "\n",
       "                                   case_templ pred_BERT_combined_weighted  \n",
       "0                        I hate [IDENTITY_P].                 non-hateful  \n",
       "1                        I hate [IDENTITY_P].                 non-hateful  \n",
       "2                        I hate [IDENTITY_P].                 non-hateful  \n",
       "3                        I hate [IDENTITY_P].                 non-hateful  \n",
       "4                        I hate [IDENTITY_P].                 non-hateful  \n",
       "...                                       ...                         ...  \n",
       "3723  [IDENTITY_P] are all complete 4ssholes.                 non-hateful  \n",
       "3724  [IDENTITY_P] are all complete 4ssholes.                 non-hateful  \n",
       "3725  [IDENTITY_P] are all complete 4ssholes.                 non-hateful  \n",
       "3726  [IDENTITY_P] are all complete 4ssholes.                 non-hateful  \n",
       "3727  [IDENTITY_P] are all complete 4ssholes.                 non-hateful  \n",
       "\n",
       "[3728 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PR: except for the pred_ column, this dataframe should be the result of \"loadHateCheck\"\n",
    "# PR: could we follow the same format as for the test sets? i.e. load HateCheck in one function, then get and concatenate model predictions on HateCheck in the next?\n",
    "hatecheck_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to dict\n",
    "func_accuracy_dict = {}\n",
    "\n",
    "for m in models_eval:\n",
    "    #print(\"m\")\n",
    "    func_accuracy_dict[m] = []\n",
    "    for func in pd.unique(hatecheck_df.functionality):\n",
    "        #print(\"func\")\n",
    "        n_cases = hatecheck_df[hatecheck_df.functionality==func].shape[0]\n",
    "        #print(n_cases)\n",
    "        #print(hatecheck_df[(hatecheck_df.functionality==func)&(hatecheck_df['label_gold']==hatecheck_df['pred_{}'.format(m)])])\n",
    "        n_correct = hatecheck_df[(hatecheck_df.functionality==func)&(hatecheck_df['label_gold']==hatecheck_df['pred_{}'.format(m)])].shape[0]\n",
    "        func_accuracy_dict[m].append('{:.1%}'.format(n_correct/n_cases))\n",
    "    \n",
    "    # convert list to series\n",
    "    func_accuracy_dict[m] = pd.Series(func_accuracy_dict[m])\n",
    "    func_accuracy_dict[m].name = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from dict\n",
    "func_accuracy_df = pd.Series(pd.unique(hatecheck_df.functionality))\n",
    "func_accuracy_df.name = 'functionality'\n",
    "\n",
    "for arc_data in func_accuracy_dict:\n",
    "    func_accuracy_df = pd.concat([func_accuracy_df, pd.Series(func_accuracy_dict[arc_data])], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionality</th>\n",
       "      <th>BERT_combined_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>8.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>derog_neg_attrib_h</td>\n",
       "      <td>27.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>derog_dehum_h</td>\n",
       "      <td>25.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derog_impl_h</td>\n",
       "      <td>47.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>threat_dir_h</td>\n",
       "      <td>20.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>threat_norm_h</td>\n",
       "      <td>17.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>slur_h</td>\n",
       "      <td>11.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>slur_homonym_nh</td>\n",
       "      <td>96.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>slur_reclaimed_nh</td>\n",
       "      <td>88.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>profanity_h</td>\n",
       "      <td>12.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>profanity_nh</td>\n",
       "      <td>98.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ref_subs_clause_h</td>\n",
       "      <td>30.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ref_subs_sent_h</td>\n",
       "      <td>19.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>negate_pos_h</td>\n",
       "      <td>48.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>negate_neg_nh</td>\n",
       "      <td>42.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_question_h</td>\n",
       "      <td>13.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_opinion_h</td>\n",
       "      <td>26.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ident_neutral_nh</td>\n",
       "      <td>40.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ident_pos_nh</td>\n",
       "      <td>74.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>counter_quote_nh</td>\n",
       "      <td>82.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>counter_ref_nh</td>\n",
       "      <td>72.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>target_obj_nh</td>\n",
       "      <td>92.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>target_indiv_nh</td>\n",
       "      <td>98.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target_group_nh</td>\n",
       "      <td>80.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spell_char_swap_h</td>\n",
       "      <td>12.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spell_char_del_h</td>\n",
       "      <td>9.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spell_space_del_h</td>\n",
       "      <td>9.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spell_space_add_h</td>\n",
       "      <td>3.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3.5%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         functionality BERT_combined_weighted\n",
       "0    derog_neg_emote_h                   8.6%\n",
       "1   derog_neg_attrib_h                  27.9%\n",
       "2        derog_dehum_h                  25.7%\n",
       "3         derog_impl_h                  47.9%\n",
       "4         threat_dir_h                  20.3%\n",
       "5        threat_norm_h                  17.1%\n",
       "6               slur_h                  11.1%\n",
       "7      slur_homonym_nh                  96.7%\n",
       "8    slur_reclaimed_nh                  88.9%\n",
       "9          profanity_h                  12.1%\n",
       "10        profanity_nh                  98.0%\n",
       "11   ref_subs_clause_h                  30.7%\n",
       "12     ref_subs_sent_h                  19.5%\n",
       "13        negate_pos_h                  48.6%\n",
       "14       negate_neg_nh                  42.9%\n",
       "15   phrase_question_h                  13.6%\n",
       "16    phrase_opinion_h                  26.3%\n",
       "17    ident_neutral_nh                  40.5%\n",
       "18        ident_pos_nh                  74.1%\n",
       "19    counter_quote_nh                  82.1%\n",
       "20      counter_ref_nh                  72.3%\n",
       "21       target_obj_nh                  92.3%\n",
       "22     target_indiv_nh                  98.5%\n",
       "23     target_group_nh                  80.6%\n",
       "24   spell_char_swap_h                  12.8%\n",
       "25    spell_char_del_h                   9.3%\n",
       "26   spell_space_del_h                   9.2%\n",
       "27   spell_space_add_h                   3.5%\n",
       "28        spell_leet_h                   3.5%"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Classification Reports for Each Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_COMBINED_WEIGHTED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " non-hateful       0.29      0.74      0.42      1165\n",
      "     hateful       0.62      0.19      0.29      2563\n",
      "\n",
      "    accuracy                           0.36      3728\n",
      "   macro avg       0.46      0.47      0.36      3728\n",
      "weighted avg       0.52      0.36      0.33      3728\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification reports for each classifier\n",
    "\n",
    "for m in models_eval:\n",
    "        print(m.upper())\n",
    "        print(classification_report(hatecheck_df.label_gold.replace({'hateful': 1, 'non-hateful':0}),\n",
    "                                    hatecheck_df['pred_{}'.format(m)].replace({'hateful': 1, 'non-hateful':0}),\n",
    "                                    target_names = ['non-hateful','hateful']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY \n",
      "\n",
      "BERT_combined_weighted\n",
      "hateful: 18.9%:\n",
      "non-hateful: 74.2%:\n",
      "TOTAL: 36.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY \\n')\n",
    "for m in models_eval:\n",
    "    print(m)\n",
    "    for lab in ['hateful', 'non-hateful']:\n",
    "        n_cases = hatecheck_df[hatecheck_df.label_gold==lab].shape[0]\n",
    "        n_correct = hatecheck_df[(hatecheck_df.label_gold==lab) & (hatecheck_df['label_gold']==hatecheck_df['pred_{}'.format(m)])].shape[0]\n",
    "        print('{}: {:.1%}:'.format(lab, n_correct/n_cases))\n",
    "    print('TOTAL: {:.1%}'.format(hatecheck_df[(hatecheck_df['label_gold']==hatecheck_df['pred_{}'.format(m)])].shape[0]/hatecheck_df.shape[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance for Reclaimed Slurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY\n",
      "BERT_COMBINED_WEIGHTED\n",
      "19\n",
      "nigga : 100.0%\n",
      "16\n",
      "fag : 87.5%\n",
      "16\n",
      "faggot : 100.0%\n",
      "15\n",
      "queer : 53.3%\n",
      "15\n",
      "bitch : 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hatecheck_df[(hatecheck_df.functionality == 'slur_reclaimed_nh')].groupby('focus_lemma').case_id.count().keys()\n",
    "\n",
    "print('ACCURACY')\n",
    "for m in models_eval:\n",
    "    print(m.upper())\n",
    "    for slur in ['nigga', 'fag', 'faggot', 'queer', 'bitch']:\n",
    "        n_total = hatecheck_df[(hatecheck_df.functionality == 'slur_reclaimed_nh')&\n",
    "                                (hatecheck_df.focus_lemma==slur)].shape[0]\n",
    "        n_correct = hatecheck_df[(hatecheck_df.functionality == 'slur_reclaimed_nh')&\n",
    "                                  (hatecheck_df['pred_{}'.format(m)]==hatecheck_df.label_gold)&\n",
    "                                  (hatecheck_df.focus_lemma==slur)].shape[0]\n",
    "        print(n_total)\n",
    "        print(slur, ': {:.1%}'.format(n_correct/n_total))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Across Cases by Target Identity\n",
    "Only uses cases generated from templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_ident\n",
       "Muslims            421\n",
       "black people       421\n",
       "disabled people    421\n",
       "gay people         421\n",
       "immigrants         421\n",
       "trans people       421\n",
       "women              421\n",
       "Name: case_id, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with only template cases --> number of cases for each identity should be balanced\n",
    "templ_cases_df = hatecheck_df[hatecheck_df.case_templ.str.contains('IDENTITY')].copy()\n",
    "\n",
    "templ_cases_df.groupby(templ_cases_df.target_ident).case_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to dict\n",
    "ident_accuracy_dict = {}\n",
    "\n",
    "for m in models_eval:\n",
    "    ident_accuracy_dict[m] = []\n",
    "    for ident in pd.unique(templ_cases_df.target_ident):\n",
    "        n_cases = templ_cases_df[templ_cases_df.target_ident==ident].shape[0]\n",
    "        n_correct = templ_cases_df[(templ_cases_df.target_ident==ident)&(templ_cases_df['label_gold']==templ_cases_df['pred_{}'.format(m)])].shape[0]\n",
    "        ident_accuracy_dict[m].append('{:.1%}'.format(n_correct/n_cases))\n",
    "    ident_accuracy_dict[m] = pd.Series(ident_accuracy_dict[m])\n",
    "    ident_accuracy_dict[m].name = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from dict\n",
    "ident_accuracy_df = pd.Series(pd.unique(templ_cases_df.target_ident))\n",
    "ident_accuracy_df.name = 'target_ident'\n",
    "\n",
    "for arc_data in ident_accuracy_dict:\n",
    "    ident_accuracy_df = pd.concat([ident_accuracy_df, pd.Series(ident_accuracy_dict[arc_data])], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_ident</th>\n",
       "      <th>BERT_combined_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>women</td>\n",
       "      <td>27.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trans people</td>\n",
       "      <td>26.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gay people</td>\n",
       "      <td>25.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>black people</td>\n",
       "      <td>26.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disabled people</td>\n",
       "      <td>39.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Muslims</td>\n",
       "      <td>36.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>immigrants</td>\n",
       "      <td>26.8%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target_ident BERT_combined_weighted\n",
       "0            women                  27.8%\n",
       "1     trans people                  26.8%\n",
       "2       gay people                  25.2%\n",
       "3     black people                  26.4%\n",
       "4  disabled people                  39.0%\n",
       "5          Muslims                  36.1%\n",
       "6       immigrants                  26.8%"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ident_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
